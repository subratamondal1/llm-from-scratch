{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Three Core Components of PyTorch\n",
    "\n",
    "<img src=\"../asssets/a1-three-components-of-pytorch.png\"/>\n",
    "\n",
    "1. **Tensor Library:** Extends the concept of array-oriented programming library, *`NumPy`* with the *`GPU`* support.\n",
    "\n",
    "2. **Automatic Differentiation Engine `(Autograd)`:** Enables Automatic Calculation of Gradients(Slopes) for Tensor Operations simplifying the process of `backpropagation` and `model optimization`.\n",
    "\n",
    "3. **Deep Learning Library:** Offers `Modular`, `Flexible` and `Efficient` Building Blocks including `Pretrained Models`, `Loss Functions` and `Optimizers` for designing and training a wide range of deep learning models.\n",
    "\n",
    "\n",
    "> In the news, **LLMs** are often referred to as **AI models**. However, LLMs are also a type of  **deep  neural  network**,  and **PyTorch**  is  a  deep  learning  library.\n",
    "\n",
    "<img src=\"../asssets/ai-ml-dl.png\"/>\n",
    "\n",
    "<img src=\"../asssets/a3-supervised-learning.png\"/>\n",
    "\n",
    "<img src=\"../asssets/apple silicon.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.0'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Tensors\n",
    "\n",
    "<img src=\"../asssets/tensors.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scalars, Vectors, Matrices and Tensors\n",
    "\n",
    "<img src=\"../asssets/create-tensors.png\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "tensor0d: torch.Tensor = torch.tensor(data=1)\n",
    "print(tensor0d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "tensor1d: torch.Tensor = torch.tensor(data=[1, 2, 3])\n",
    "print(tensor1d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2],\n",
      "        [3, 4]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "tensor2d: torch.Tensor = torch.tensor(data=[[1, 2], [3, 4]])\n",
    "print(tensor2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1, 2],\n",
      "         [3, 4]],\n",
      "\n",
      "        [[5, 6],\n",
      "         [7, 8]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "tensor3d: torch.Tensor = torch.tensor(\n",
    "    data=[[[1, 2], [3, 4]], [[5, 6], [7, 8]]],\n",
    ")\n",
    "print(tensor3d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor Datatypes\n",
    "\n",
    "- PyTorch  adopts  the  default  `64-bit  integer`  data  type  from  Python.  We  can  access  the data type of a tensor via the *`.dtype`* attribute of a tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3])\n",
      "torch.int64\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "tensor1d: torch.Tensor = torch.tensor(data=[1, 2, 3])\n",
    "print(tensor1d)\n",
    "print(tensor1d.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If we create tensors from Python floats, PyTorch creates tensors with a *`32-bit precision`* by default:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3.])\n",
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "floatvector: torch.Tensor = torch.tensor(data=[1.0, 2.0, 3.0])\n",
    "print(floatvector)\n",
    "print(floatvector.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This choice is primarily due to the *balance between precision and computational efficiency*. A 32-bit floating-point number offers sufficient precision for most deep learning tasks while consuming less memory and computational resources than a 64-bit floating-point number. Moreover, *GPU architectures are optimized for 32-bit* computations, and using this data type can significantly speed up model training and inference.**\n",
    "\n",
    "> Moreover, it is possible to change the precision using a tensor’s **`.to`** method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3])\n",
      "torch.int64 \n",
      "\n",
      "tensor([1., 2., 3.])\n",
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "floatvector64: torch.Tensor = torch.tensor(data=[1, 2, 3])\n",
    "print(floatvector64)\n",
    "print(floatvector64.dtype, \"\\n\")\n",
    "\n",
    "floatvector32: torch.Tensor = torch.tensor(data=[1, 2, 3]).to(dtype=torch.float32)\n",
    "print(floatvector32)\n",
    "print(floatvector32.dtype)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common PyTorch Tensor Operations\n",
    "\n",
    "- The *`.shape`* attribute allows us to access the shape of a tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "tensor2d: torch.Tensor = torch.tensor(data=[[1, 2, 3], [4, 5, 6]])\n",
    "print(tensor2d)\n",
    "print(tensor2d.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, *`.shape`* returns `[2, 3]`, meaning the tensor has *2 rows* and *3 columns*. To reshape the tensor into a `3 × 2` tensor, we can use the *`.reshape`* method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2],\n",
      "        [3, 4],\n",
      "        [5, 6]])\n"
     ]
    }
   ],
   "source": [
    "print(tensor2d.reshape(3, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, note that the *more common command for reshaping* tensors in PyTorch is *`.view()`*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2],\n",
      "        [3, 4],\n",
      "        [5, 6]])\n"
     ]
    }
   ],
   "source": [
    "print(tensor2d.view(3, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key difference between `.view()` and `.reshape()` in PyTorch lies in how they handle memory layout: `.view()` requires the tensor to be **contiguous** (data stored in a continuous block of memory) and will raise an error if it isn’t, as it only *provides a new \"view\" into the existing data* **without copying it**. In contrast, `.reshape()` works regardless of whether the tensor is contiguous; if needed, it creates a new, contiguous copy of the data to ensure the desired shape. Use `.view()` for efficiency when the tensor is contiguous and `.reshape()` for flexibility.\n",
    "\n",
    "\n",
    "- We can use **`.T`** to transpose a tensor, which means flipping it across its diagonal. Note that this is similar to reshaping a tensor, as you can see based on the following result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "tensor([[1, 4],\n",
      "        [2, 5],\n",
      "        [3, 6]])\n"
     ]
    }
   ],
   "source": [
    "print(tensor2d)\n",
    "print(tensor2d.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The common way to multiply two matrices in PyTorch is the **`.matmul`** method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "tensor([[1, 4],\n",
      "        [2, 5],\n",
      "        [3, 6]])\n",
      "tensor([[14, 32],\n",
      "        [32, 77]])\n"
     ]
    }
   ],
   "source": [
    "print(tensor2d)\n",
    "print(tensor2d.T)\n",
    "print(tensor2d.matmul(other=tensor2d.T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also adopt the **`@`** operator, which accomplishes the same thing more compactly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "tensor([[1, 4],\n",
      "        [2, 5],\n",
      "        [3, 6]])\n",
      "tensor([[14, 32],\n",
      "        [32, 77]])\n"
     ]
    }
   ],
   "source": [
    "print(tensor2d)\n",
    "print(tensor2d.T)\n",
    "print(tensor2d @ tensor2d.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seeing Models as Computational Graphs\n",
    "Now let’s look at PyTorch’s *`automatic differentiation engine`*, also known as *`autograd`*. PyTorch’s autograd system provides *functions to compute gradients (slopes)* in dynamic computational graphs automatically. \n",
    "\n",
    "- A **`computational graph`** is a `directed graph` that allows us to **express** and **visualize mathematical expressions**. In the context of deep learning, a computation graph lays out the sequence of calculations needed to compute the output of a neural network we  will  need  this  to  compute  the  required  gradients  for backpropagation,  the  main training algorithm for neural networks.\n",
    "\n",
    "The code in the following listing implements the **forward pass (prediction step)** of a **simple logistic regression classifier**, which can be seen as a `single-layer neural network`. It returns a score between 0 and 1, which is compared to the true class label (0 or 1) when computing the loss.\n",
    "\n",
    "\n",
    "<img src=\"../asssets/logistic-regression-forward-pass.png\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "y: torch.Tensor = torch.tensor(data=[1.0])  # True Label\n",
    "\n",
    "x1: torch.Tensor = torch.tensor(data=[1.1])  # Indepndent Variable\n",
    "w1: torch.Tensor = torch.tensor(data=[2.2])  # Weight\n",
    "\n",
    "b: torch.Tensor = torch.tensor(data=[0.0])  # Bias\n",
    "\n",
    "z: torch.Tensor = x1 * w1 + b  # Linear Function\n",
    "a: torch.Tensor = torch.sigmoid(input=z)  # Activation Function\n",
    "\n",
    "loss: torch.Tensor = F.binary_cross_entropy(input=a, target=y)  # Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../asssets/computational-graph.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch builds such a computation graph in the background, and we can use this to *`calculate gradients(slope) of a loss function with respect to the model parameters`* (here **`w1`** and **`b`**) *`to train the model.`*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic Differentiation Made Easy\n",
    "If we carry out computations in PyTorch, it will build a computational graph internally by default if one of its terminal  nodes has the **`requires_grad`** attribute  set to `True`. This is useful if we want to compute gradients. **Gradients are required when training neural networks** via the popular **`backpropagation algorithm`**, which can be considered an *`implementation of the chain rule`* from calculus for neural networks.\n",
    "\n",
    "<img src=\"../asssets/partial-derivative.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PARTIAL DERIVATIVES AND GRADIENTS\n",
    "\n",
    "- **`Partial Derivatives:`** measure *`the rate at which a function changes with respect to one of its variables`*. \n",
    "\n",
    "- A **`gradient (slope)`** is a *`vector containing all of the partial derivatives of a multivariate function`*, *a function with more than one variable as input*.\n",
    "\n",
    "\n",
    "> On a high level, the **`Chain Rule`** is a way to *`compute the gradients (slope) of a Loss Function`* given the Model's Parameters in a Computational Graph. This provides the information needed to **update** each of the Model's Parameter to **Minimize the Loss Functi**, which serves as a Proxy for measuring the **Performance** of the Model using **Gradient Descent**.\n",
    "\n",
    "**So, Why Autograd?**\n",
    "\n",
    "- It **automatically** builds a `computational graph` for us. How? By tracking every operation performed on Tensors.\n",
    "- It **automatically** computes the `gradients (slopes)` for us. How? By calling the **`grad`** function, we can compute the `gradients of a loss function` with respect to the model parameters (*`weights and biases`*).\n",
    "\n",
    "<img src=\"../asssets/compute-gradients-with-autograd.png\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([-0.0898]),)\n",
      "(tensor([-0.0817]),)\n",
      "tensor([-0.0898]) tensor([-0.0817])\n"
     ]
    }
   ],
   "source": [
    "from typing import Tuple\n",
    "from torch import Tensor\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import grad\n",
    "\n",
    "y: Tensor = torch.tensor(data=[1.0])  # True Label\n",
    "\n",
    "x1: Tensor = torch.tensor(data=[1.1])  # Indepndent Variable\n",
    "w1: Tensor = torch.tensor(data=[2.2], requires_grad=True)  # Weight\n",
    "\n",
    "b: Tensor = torch.tensor(data=[0.0], requires_grad=True)  # Bias\n",
    "\n",
    "z: Tensor = x1 * w1 + b  # Linear Function\n",
    "a: Tensor = torch.sigmoid(input=z)  # Activation Function\n",
    "\n",
    "loss: Tensor = F.binary_cross_entropy(input=a, target=y)  # Loss\n",
    "\n",
    "gradients_of_loss_wrt_w1: Tuple[Tensor, ...] = grad(\n",
    "    outputs=loss,\n",
    "    inputs=w1,\n",
    "    retain_graph=True,\n",
    ")\n",
    "gradients_of_loss_wrt_b: Tuple[Tensor, ...] = grad(\n",
    "    outputs=loss,\n",
    "    inputs=b,\n",
    "    retain_graph=True,\n",
    ")\n",
    "\n",
    "print(gradients_of_loss_wrt_w1)\n",
    "print(gradients_of_loss_wrt_b)\n",
    "\n",
    "\n",
    "# More efficient and compact way to compute gradients of the Loss Function with respect to the Model's Parameters\n",
    "loss.backward()  # Calculates the Gradients of the Loss Function wrt all those Tensors that have requires_grad=True\n",
    "print(w1.grad, b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*While calling `loss.backward()` how does pytorch knows to calculate the `gradients of` **`loss`** wrt whom?*\n",
    "\n",
    "When you call `loss.backward()`, it calculates the gradients of the loss with respect to all those tensors that have `requires_grad=True`.\n",
    "\n",
    "> **Note:** While the Calculus Jargon is a means to explain PyTorch's *`autograd`* component , all we need to take away is the PyTorch takes care of the Calculus for us via the **`.backward()`** method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Multilayer Neural Networks\n",
    "\n",
    "> Now we focus on PyTorch as a library for implementing Deep Neural Networks.\n",
    "\n",
    "<img src=\"../asssets/multilayer-perceptron.png\"/>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Each Layer can have Multiple Nodes.\n",
    "\n",
    "When implementing a Neural Network in PyTorch, we can `subclass` the **`torch.nn.Module`** `class` to define our own Custom Network Architecture. This *`Module`* Base Class provides a lot of functionality, making it easier to Build and Train Models. For example, it allows us to *`Encapsualte Layers`* and *`Operations`* and *`Keep Track of the Model's Parameters`*.\n",
    "\n",
    "Within this Sub-Class, we *`define the Network Layers`* in the *`__init__ constructor`* and specify *`how the Layers interact in the Forward Method`*. The *`Forward Method`* `describes how the Input Data passes through the Network` and `comes together as a Computation Graph.` And the *`Backward Method`* `computes the Gradients of the Loss Function with respect to the Model's Parameters (weights & biases) during Training.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import torch\n",
    "\n",
    "class Model(torch.nn.Module):\n",
    "    pass\n",
    "```\n",
    "\n",
    "**Inherited Methods:** \n",
    "The class inherits numerous methods from the `Module` base class, including:\n",
    "- `forward()`: Defines the computation performed at every call.\n",
    "\n",
    "- `parameters()`: Returns an iterator over module parameters.\n",
    "- `state_dict()`: Returns a dictionary containing the module's state.\n",
    "- `load_state_dict()`: Copies parameters and buffers from a state dict.\n",
    "- `to()`: Moves and/or casts the parameters and buffers.\n",
    "- `cuda()`, `cpu()`: Moves all model parameters and buffers to the GPU/CPU.\n",
    "- `train()`, `eval()`: Sets the module in training/evaluation mode.\n",
    "- Many other utility methods for registering hooks, buffers, and parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../asssets/multilayer-perceptron-with-2-hidden-layers.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (layers): Sequential(\n",
      "    (0): Linear(in_features=50, out_features=30, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=30, out_features=20, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=20, out_features=3, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "class NeuralNetwork(torch.nn.Module):\n",
    "    def __init__(self, number_of_inputs: int, number_of_outputs: int) -> None:\n",
    "        super().__init__()\n",
    "        self.layers = torch.nn.Sequential(\n",
    "            # 1st Hidden Layer\n",
    "            torch.nn.Linear(in_features=number_of_inputs, out_features=30),\n",
    "            torch.nn.ReLU(),\n",
    "            # 2nd Hidden Layer\n",
    "            torch.nn.Linear(in_features=30, out_features=20),\n",
    "            torch.nn.ReLU(),\n",
    "            # Output Layer\n",
    "            torch.nn.Linear(in_features=20, out_features=number_of_outputs),\n",
    "        )\n",
    "\n",
    "    # Shape of the Input Tensor 'x': [Batch Size, Number of Inputs]\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        logits = self.layers(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "model = NeuralNetwork(number_of_inputs=50, number_of_outputs=3)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note:** that we used the *`Sequential Class`* when defining our NeuralNetwork Class because it makes our life easier if we have a series of Layers we want to execute in a specific order, as we are doing here. This way, after instantiating *`self.layers = torch.nn.Sequential()`*, in the *`__init__`* constructor, we just have to now call the *`self.layers`* attribute instead of calling each layer individually in the *`NeuralNetwork's forward method`*.\n",
    "\n",
    "To check the *`Total Number of Trainable Parameters`* in our Model, we can use the `parameters()` Method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.nn.parameter.Parameter'> 1500\n",
      "<class 'torch.nn.parameter.Parameter'> 30\n",
      "<class 'torch.nn.parameter.Parameter'> 600\n",
      "<class 'torch.nn.parameter.Parameter'> 20\n",
      "<class 'torch.nn.parameter.Parameter'> 60\n",
      "<class 'torch.nn.parameter.Parameter'> 3\n",
      "2213\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for param in model.parameters():\n",
    "    print(type(param), param.numel())\n",
    "    count += param.numel()\n",
    "\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2213\n"
     ]
    }
   ],
   "source": [
    "total_number_of_parameters: int = sum(\n",
    "    p.numel() for p in model.parameters() if p.requires_grad\n",
    ")\n",
    "print(total_number_of_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - **Note:** Each parameter for which `requires_grad=True` counts as one `trainable parameter`.\n",
    "> - For each parameter *`p`*, the method *`.numel()`* returns the total number of elements in the tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of our Neural Network Model with the preceding two hidden layers, these *`trainable parameters`* can be found in the *`torch.nn.Linear`* layers. **`A Linear layer multiplies the inputs with a weight matrix and adds a bias vector.`** This is sometimes referred to as a **`feedforward`** or **`fully connected layer`**. Based on the `print(model)` call we executed here, we can see that the `first Linear layer  is  at  index  position  0` ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (layers): Sequential(\n",
      "    (0): Linear(in_features=50, out_features=30, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=30, out_features=20, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=20, out_features=3, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=50, out_features=30, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=30, out_features=20, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=20, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model.layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.0502,  0.0307,  0.0333,  ...,  0.0951,  0.1134, -0.0297],\n",
      "        [ 0.1077, -0.1108,  0.0122,  ...,  0.0108, -0.1049, -0.1063],\n",
      "        [-0.0920, -0.0480,  0.0105,  ..., -0.0923,  0.1201,  0.0330],\n",
      "        ...,\n",
      "        [ 0.1359,  0.0175, -0.0673,  ...,  0.0674,  0.0676,  0.1058],\n",
      "        [ 0.0790,  0.1343, -0.0293,  ...,  0.0344, -0.0971, -0.0509],\n",
      "        [-0.1250,  0.0513,  0.0366,  ..., -0.1370,  0.1074, -0.0704]],\n",
      "       requires_grad=True)\n",
      "torch.Size([30, 50])\n"
     ]
    }
   ],
   "source": [
    "print(model.layers[0].weight)\n",
    "print(model.layers[0].weight.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weight matrix here is `[30 x 50]` matrix, and we can see that `requires_grad` is set to `True`, which means its entries are *`trainable`*, this is the default setting for weights and biases in *`torch.nn.Linear`*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([-1.3122e-01, -1.0667e-02,  1.0314e-01, -1.8104e-02,  3.1523e-02,\n",
      "        -1.3763e-01,  1.1643e-01,  9.1198e-02,  9.8382e-02,  7.3479e-02,\n",
      "        -7.2337e-02, -1.1853e-01,  5.3997e-04,  7.5849e-02, -6.1513e-02,\n",
      "         3.5053e-02, -1.1154e-02, -1.3147e-02,  3.6492e-02,  1.0322e-01,\n",
      "         2.9582e-02,  1.0176e-02,  2.2896e-02,  2.6020e-02, -4.5835e-02,\n",
      "        -1.6127e-02,  3.4467e-02, -1.1141e-01,  9.3445e-05, -1.3079e-01],\n",
      "       requires_grad=True)\n",
      "torch.Size([30])\n"
     ]
    }
   ],
   "source": [
    "print(model.layers[0].bias)\n",
    "print(model.layers[0].bias.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FORWARD PASS:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0879,  0.1729,  0.1534]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# NOT OPTIMIZED\n",
    "import torch\n",
    "from torch import Tensor\n",
    "\n",
    "torch.manual_seed(seed=123)\n",
    "\n",
    "X: Tensor = torch.rand(size=(1, 50))\n",
    "output: Tensor = model(X)  # automatically executes the FORWARD PASS\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *`FORWARD PASS`* refers to calculating the output tensors of a neural network from the input tensors. Works by passing the input data through all the layers, starting from the input layer, through hidden layers, and finally to the output layer.\n",
    "\n",
    "\n",
    "*`These three numbers returned here correspond to a score assigned to each of the three output nodes.`* Notice that the output tensor also includes a **`grad_fn`** value.\n",
    "\n",
    "**`grad_fn=<AddmmBackward0>`** represents the last used function to compute variable in the computational graph. It means that the tensor we are inspecting was created via Matrix Multipication and Addition Operation. PyTorch will use this information to compute the Gradients of the Loss Function with respect to the Model's Parameters suring **`BackPropagation`**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we just want to use a Network without training or backpropagation, for example, if we use it for prediction after training the model, then constructing this computational graph for backpropagation can be wasteful as it performs unnecessary computations and consumes additional memory. So, when we use model for Prediction(Inference) then the best practice is to us the `torch.no_grad` Context Manager. This tells PyTorch that it doesn't need to keep track of the gradients, which can result in significant savings in memory and computation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0879,  0.1729,  0.1534]])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    output: Tensor = model(X)  # automatically executes the FORWARD PASS\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - **`Logits = Output of the Last Layer`**\n",
    "\n",
    "In PyTorch, it's common to code models such that they return the outputs of the *`Last Layer (Logits)`* without passing them through *`Non-Linear Activation Function`*. That's because PyTorch's commonly used Loss Functions combine the Softmax (Sigmoid for Binary Classification) operation with the negative log-likelihood loss in a single class. The reason for this is `Numerical Efficiency` and `Stability`. So, if we want to compute Class-Membership Probabilities for our Predictions, we have to call the Softmax Function explicitly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 50])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2801, 0.3635, 0.3565]])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    out: Tensor = torch.softmax(input=model(X), dim=1)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values can now be interpreted as Class-Membership Probabilities that sum up to 1. The values are roughly equal for this random input, which is expected for a randomly initialized model without training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up efficient data loaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../asssets/dataloaders.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will implement a custom *`Dataset Class`*, which we will use to create a training and a test dataset that we'll then use to create a *training* and *test dataset* that we'll then use to create the dataloaders.\n",
    "\n",
    "- Creating a simple toy dataset of 5 `training set` examples with two features each. \n",
    "- Also creating a tensor containing corresponding class labels: 3 examples belong to class 0 and 2 examples belong to class 1. \n",
    "- Also creating `testing set` consisting of 2 entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train: Tensor = torch.tensor(\n",
    "    data=[\n",
    "        [-1.2, 3.1],\n",
    "        [-0.9, 2.9],\n",
    "        [-0.5, 2.6],\n",
    "        [2.3, -1.1],\n",
    "        [2.7, -1.5],\n",
    "    ]\n",
    ")\n",
    "\n",
    "y_train: Tensor = torch.tensor(\n",
    "    data=[\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        1,\n",
    "        1,\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test: Tensor = torch.tensor(\n",
    "    data=[\n",
    "        [-0.8, 2.8],\n",
    "        [2.6, -1.6],\n",
    "    ]\n",
    ")\n",
    "\n",
    "y_test: Tensor = torch.tensor(\n",
    "    data=[\n",
    "        0,\n",
    "        1,\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **`NOTE:`** **PyTorch requires that class labels start with label 0, and the largest class label value should not exceed the number of output nodes minus 1 because Python indexing starts from Zero.**\n",
    "\n",
    "- For, example if we have Class Labels `0,1,2,3` and `4`, the Neural Network Output Layer must consist of 5 Nodes only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.2000,  3.1000],\n",
      "        [-0.9000,  2.9000],\n",
      "        [-0.5000,  2.6000],\n",
      "        [ 2.3000, -1.1000],\n",
      "        [ 2.7000, -1.5000]])\n",
      "tensor([0, 0, 0, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class ToyDataset(Dataset):\n",
    "    def __init__(self, X: Tensor, y: Tensor) -> None:\n",
    "        super().__init__()\n",
    "        self.features: Tensor = X\n",
    "        self.labels: Tensor = y\n",
    "\n",
    "    def __getitem__(self, index) -> tuple[Tensor, Tensor]:\n",
    "        one_X: Tensor = self.features[index]\n",
    "        one_y: Tensor = self.labels[index]\n",
    "        return one_X, one_y\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.labels.shape[0]\n",
    "\n",
    "\n",
    "train_ds = ToyDataset(X=X_train, y=y_train)\n",
    "test_ds = ToyDataset(X=X_test, y=y_test)\n",
    "\n",
    "print(train_ds.features)\n",
    "print(train_ds.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of the `ToyDataset Class` is to instantiate a `PyTorch DataLoader`. The 3 main components of a **Custom `Dataset` Class** are the:\n",
    "- **`__init__`** constructor \n",
    "- **`__getitem__` method** here, we defined to retrieve exactly one data from features and labels.\n",
    "- **`__len__` method** here, we retrieve the length of the dataset.\n",
    "\n",
    "These could be file paths, file objects, database connectors and so on. Since, we created a tensor dataset that sits in memory, we simply assign *`X`* and *`y`* to these attributes, which are placeholders for our tensor objects.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(len(train_ds))\n",
    "print(len(test_ds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../asssets/instantiating data loaders.png\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1: tensor([[ 2.3000, -1.1000],\n",
      "        [-0.9000,  2.9000]]) | tensor([1, 0])\n",
      "Batch 2: tensor([[-1.2000,  3.1000],\n",
      "        [-0.5000,  2.6000]]) | tensor([0, 0])\n",
      "Batch 3: tensor([[ 2.7000, -1.5000]]) | tensor([1])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "torch.manual_seed(seed=123)\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    dataset=train_ds,\n",
    "    batch_size=2,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    ")\n",
    "\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    dataset=test_ds,\n",
    "    batch_size=2,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    ")\n",
    "\n",
    "\n",
    "for idx, (x, y) in enumerate(iterable=train_dataloader):\n",
    "    print(f\"Batch {idx+1}: {x} | {y}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We specified the Batch Size of 2 here, but the 3rd batch only contains a single example. That's because we have 5 training examples, and 5 is not evenly divisible by 2. \n",
    "\n",
    "> **`Note`** that in practice, having a substantially smaller batch as the last batch in a Training Epoch can disturb the Convergence during Training . To prevent this, we set **`drop_last=True`**, which will drop the last batch in Each Epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    dataset=train_ds,\n",
    "    batch_size=2,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    drop_last=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, iterating over the train dataloader, we can see that the last batch is omitted. Previously it had 3 batch now it has 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1: tensor([[-0.9000,  2.9000],\n",
      "        [ 2.3000, -1.1000]]) | tensor([0, 1])\n",
      "Batch 2: tensor([[ 2.7000, -1.5000],\n",
      "        [-0.5000,  2.6000]]) | tensor([1, 0])\n"
     ]
    }
   ],
   "source": [
    "for idx, (x, y) in enumerate(iterable=train_dataloader):\n",
    "    print(f\"Batch {idx+1}: {x} | {y}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-from-scratch-GfHW_nTs-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
