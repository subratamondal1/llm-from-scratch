{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Three Core Components of PyTorch\n",
    "\n",
    "<img src=\"../asssets/a1-three-components-of-pytorch.png\"/>\n",
    "\n",
    "1. **Tensor Library:** Extends the concept of array-oriented programming library, *`NumPy`* with the *`GPU`* support.\n",
    "\n",
    "2. **Automatic Differentiation Engine `(Autograd)`:** Enables Automatic Calculation of Gradients for Tensor Operations simplifying the process of backpropagation and model optimization.\n",
    "\n",
    "3. **Deep Learning Library:** Offers Modular, Flexible and Efficient Building Blocks including Pretrained Models, Loss Functions and Optimizers for designing and training a wide range of deep learning models.\n",
    "\n",
    "\n",
    "> In the news, **LLMs** are often referred to as **AI models**. However, LLMs are also a type of  **deep  neural  network**,  and **PyTorch**  is  a  deep  learning  library.\n",
    "\n",
    "<img src=\"../asssets/ai-ml-dl.png\"/>\n",
    "\n",
    "<img src=\"../asssets/a3-supervised-learning.png\"/>\n",
    "\n",
    "<img src=\"../asssets/apple silicon.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.0'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Tensors\n",
    "\n",
    "<img src=\"../asssets/tensors.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scalars, Vectors, Matrices and Tensors\n",
    "\n",
    "<img src=\"../asssets/create-tensors.png\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "tensor0d: torch.Tensor = torch.tensor(data=1)\n",
    "print(tensor0d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "tensor1d: torch.Tensor = torch.tensor(data=[1, 2, 3])\n",
    "print(tensor1d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2],\n",
      "        [3, 4]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "tensor2d: torch.Tensor = torch.tensor(data=[[1, 2], [3, 4]])\n",
    "print(tensor2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1, 2],\n",
      "         [3, 4]],\n",
      "\n",
      "        [[5, 6],\n",
      "         [7, 8]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "tensor3d: torch.Tensor = torch.tensor(\n",
    "    data=[[[1, 2], [3, 4]], [[5, 6], [7, 8]]],\n",
    ")\n",
    "print(tensor3d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor Datatypes\n",
    "\n",
    "- PyTorch  adopts  the  default  `64-bit  integer`  data  type  from  Python.  We  can  access  the data type of a tensor via the *`.dtype`* attribute of a tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3])\n",
      "torch.int64\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "tensor1d: torch.Tensor = torch.tensor(data=[1, 2, 3])\n",
    "print(tensor1d)\n",
    "print(tensor1d.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If we create tensors from Python floats, PyTorch creates tensors with a *`32-bit precision`* by default:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3.])\n",
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "floatvector: torch.Tensor = torch.tensor(data=[1.0, 2.0, 3.0])\n",
    "print(floatvector)\n",
    "print(floatvector.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This choice is primarily due to the *balance between precision and computational efficiency*. A 32-bit floating-point number offers sufficient precision for most deep learning tasks while consuming less memory and computational resources than a 64-bit floating-point number. Moreover, *GPU architectures are optimized for 32-bit* computations, and using this data type can significantly speed up model training and inference.**\n",
    "\n",
    "> Moreover, it is possible to change the precision using a tensor’s **`.to`** method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3])\n",
      "torch.int64 \n",
      "\n",
      "tensor([1., 2., 3.])\n",
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "floatvector64: torch.Tensor = torch.tensor(data=[1, 2, 3])\n",
    "print(floatvector64)\n",
    "print(floatvector64.dtype, \"\\n\")\n",
    "\n",
    "floatvector32: torch.Tensor = torch.tensor(data=[1, 2, 3]).to(dtype=torch.float32)\n",
    "print(floatvector32)\n",
    "print(floatvector32.dtype)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common PyTorch Tensor Operations\n",
    "\n",
    "- The *`.shape`* attribute allows us to access the shape of a tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "tensor2d: torch.Tensor = torch.tensor(data=[[1, 2, 3], [4, 5, 6]])\n",
    "print(tensor2d)\n",
    "print(tensor2d.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, *`.shape`* returns `[2, 3]`, meaning the tensor has *2 rows* and *3 columns*. To reshape the tensor into a `3 × 2` tensor, we can use the *`.reshape`* method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2],\n",
      "        [3, 4],\n",
      "        [5, 6]])\n"
     ]
    }
   ],
   "source": [
    "print(tensor2d.reshape(3, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, note that the *more common command for reshaping* tensors in PyTorch is *`.view()`*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2],\n",
      "        [3, 4],\n",
      "        [5, 6]])\n"
     ]
    }
   ],
   "source": [
    "print(tensor2d.view(3, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key difference between `.view()` and `.reshape()` in PyTorch lies in how they handle memory layout: `.view()` requires the tensor to be **contiguous** (data stored in a continuous block of memory) and will raise an error if it isn’t, as it only *provides a new \"view\" into the existing data* **without copying it**. In contrast, `.reshape()` works regardless of whether the tensor is contiguous; if needed, it creates a new, contiguous copy of the data to ensure the desired shape. Use `.view()` for efficiency when the tensor is contiguous and `.reshape()` for flexibility.\n",
    "\n",
    "\n",
    "- We can use **`.T`** to transpose a tensor, which means flipping it across its diagonal. Note that this is similar to reshaping a tensor, as you can see based on the following result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "tensor([[1, 4],\n",
      "        [2, 5],\n",
      "        [3, 6]])\n"
     ]
    }
   ],
   "source": [
    "print(tensor2d)\n",
    "print(tensor2d.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The common way to multiply two matrices in PyTorch is the **`.matmul`** method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "tensor([[1, 4],\n",
      "        [2, 5],\n",
      "        [3, 6]])\n",
      "tensor([[14, 32],\n",
      "        [32, 77]])\n"
     ]
    }
   ],
   "source": [
    "print(tensor2d)\n",
    "print(tensor2d.T)\n",
    "print(tensor2d.matmul(other=tensor2d.T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also adopt the **`@`** operator, which accomplishes the same thing more compactly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "tensor([[1, 4],\n",
      "        [2, 5],\n",
      "        [3, 6]])\n",
      "tensor([[14, 32],\n",
      "        [32, 77]])\n"
     ]
    }
   ],
   "source": [
    "print(tensor2d)\n",
    "print(tensor2d.T)\n",
    "print(tensor2d @ tensor2d.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seeing Models as Computational Graphs\n",
    "Now let’s look at PyTorch’s *`automatic differentiation engine`*, also known as *`autograd`*. PyTorch’s autograd system provides *functions to compute gradients* in dynamic computational graphs automatically. \n",
    "\n",
    "- A **`computational graph`** is a `directed graph` that allows us to **express** and **visualize mathematical expressions**. In the context of deep learning, a computation graph lays out the sequence of calculations needed to compute the output of a neural network we  will  need  this  to  compute  the  required  gradients  for backpropagation,  the  main training algorithm for neural networks.\n",
    "\n",
    "The code in the following listing implements the **forward pass (prediction step)** of a **simple logistic regression classifier**, which can be seen as a `single-layer neural network`. It returns a score between 0 and 1, which is compared to the true class label (0 or 1) when computing the loss.\n",
    "\n",
    "\n",
    "<img src=\"../asssets/logistic-regression-forward-pass.png\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "y: torch.Tensor = torch.tensor(data=[1.0])  # True Label\n",
    "\n",
    "x1: torch.Tensor = torch.tensor(data=[1.1])  # Indepndent Variable\n",
    "w1: torch.Tensor = torch.tensor(data=[2.2])  # Weight\n",
    "\n",
    "b: torch.Tensor = torch.tensor(data=[0.0])  # Bias\n",
    "\n",
    "z: torch.Tensor = x1 * w1 + b  # Linear Function\n",
    "a: torch.Tensor = torch.sigmoid(input=z)  # Activation Function\n",
    "\n",
    "loss: torch.Tensor = F.binary_cross_entropy(input=a, target=y)  # Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../asssets/computational-graph.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch builds such a computation graph in the background, and we can use this to *`calculate gradients(slope) of a loss function with respect to the model parameters`* (here **`w1`** and **`b`**) *`to train the model.`*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic Differentiation Made Easy\n",
    "If we carry out computations in PyTorch, it will build a computational graph internally by default if one of its terminal  nodes has the **`requires_grad`** attribute  set to `True`. This is useful if we want to compute gradients. **Gradients are required when training neural networks** via the popular **`backpropagation algorithm`**, which can be considered an *`implementation of the chain rule`* from calculus for neural networks.\n",
    "\n",
    "<img src=\"../asssets/partial-derivative.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PARTIAL DERIVATIVES AND GRADIENTS\n",
    "\n",
    "- **`Partial Derivatives:`** measure *`the rate at which a function changes with respect to one of its variables`*. \n",
    "\n",
    "- A **`gradient (slope)`** is a *`vector containing all of the partial derivatives of a multivariate function`*, *a function with more than one variable as input*.\n",
    "\n",
    "\n",
    "> On a high level, the Chain Rule is a way to compute the gradients (slope) of a Loss Function given the Model's Parameters in a Computational Graph. This provides the information needed to **update** each of the Model's Parameter to **Minimize** the Loss Function, which serves as a Proxy for measuring the **Performance** of the Model using **Gradient Descent**.\n",
    "\n",
    "**So, Why Autograd?**\n",
    "\n",
    "- It **automatically** builds a `computational graph` for us. How? By tracking every operation performed on Tensors.\n",
    "- It **automatically** computes the `gradients` for us. How? By calling the **`grad`** function, we can compute the `gradients of a loss function` with respect to the model parameters (weights and biases).\n",
    "\n",
    "<img src=\"../asssets/compute-gradients-with-autograd.png\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([-0.0898]),)\n",
      "(tensor([-0.0817]),)\n",
      "tensor([-0.0898]) tensor([-0.0817])\n"
     ]
    }
   ],
   "source": [
    "from typing import Tuple\n",
    "from torch import Tensor\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import grad\n",
    "\n",
    "y: Tensor = torch.tensor(data=[1.0])  # True Label\n",
    "\n",
    "x1: Tensor = torch.tensor(data=[1.1])  # Indepndent Variable\n",
    "w1: Tensor = torch.tensor(data=[2.2], requires_grad=True)  # Weight\n",
    "\n",
    "b: Tensor = torch.tensor(data=[0.0], requires_grad=True)  # Bias\n",
    "\n",
    "z: Tensor = x1 * w1 + b  # Linear Function\n",
    "a: Tensor = torch.sigmoid(input=z)  # Activation Function\n",
    "\n",
    "loss: Tensor = F.binary_cross_entropy(input=a, target=y)  # Loss\n",
    "\n",
    "gradients_of_loss_wrt_w1: Tuple[Tensor, ...] = grad(\n",
    "    outputs=loss,\n",
    "    inputs=w1,\n",
    "    retain_graph=True,\n",
    ")\n",
    "gradients_of_loss_wrt_b: Tuple[Tensor, ...] = grad(\n",
    "    outputs=loss,\n",
    "    inputs=b,\n",
    "    retain_graph=True,\n",
    ")\n",
    "\n",
    "print(gradients_of_loss_wrt_w1)\n",
    "print(gradients_of_loss_wrt_b)\n",
    "\n",
    "\n",
    "# More efficient and compact way to compute gradients of the Loss Function with respect to the Model's Parameters\n",
    "loss.backward()  # Calculates the Gradients of the Loss Function wrt all those Tensors that have requires_grad=True\n",
    "print(w1.grad, b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*While calling `loss.backward()` how does pytorch knows to calculate the `gradients of` **`loss`** wrt whom?*\n",
    "\n",
    "When you call `loss.backward()`, it calculates the gradients of the loss with respect to all those tensors that have `requires_grad=True`.\n",
    "\n",
    "> **Note:** While the Calculus Jargon is a means to explain PyTorch's *`autograd`* component , all we need to take away is the PyTorch takes care of the Calculus for us via the **`.backward()`** method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing Multilayer Neural Networks\n",
    "\n",
    "> Now we focus on PyTorch as a library for implementing Deep Neural Networks.\n",
    "\n",
    "<img src=\"../asssets/multilayer-perceptron.png\"/>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Each Layer can have Multiple Nodes.\n",
    "\n",
    "When implementing a Neural Network in PyTorch, we can `subclass` the **`torch.nn.Module`** `class` to define our own Custom Network Architecture. This *`Module`* Base Class provides a lot of functionality, making it easier to Build and Train Models. For example, it allows us to *`Encapsualte Layers`* and *`Operations`* and *`Keep Track of the Model's Parameters`*.\n",
    "\n",
    "Within this Sub Class, we *`define the Network Layers`* in the *`__init__ constructor`* and specify *`how the Layers interact in the Forward Method`*. The *`Forward Method`* `describes how the Input Data passes through the Network` and `comes together as a Computation Graph.` And the *`Backward Method`* `computes the Gradients of the Loss Function with respect to the Model's Parameters during Training.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import torch\n",
    "\n",
    "class Model(torch.nn.Module):\n",
    "    pass\n",
    "```\n",
    "\n",
    "**Inherited Methods:** \n",
    "The class inherits numerous methods from the `Module` base class, including:\n",
    "- `forward()`: Defines the computation performed at every call.\n",
    "\n",
    "- `parameters()`: Returns an iterator over module parameters.\n",
    "- `state_dict()`: Returns a dictionary containing the module's state.\n",
    "- `load_state_dict()`: Copies parameters and buffers from a state dict.\n",
    "- `to()`: Moves and/or casts the parameters and buffers.\n",
    "- `cuda()`, `cpu()`: Moves all model parameters and buffers to the GPU/CPU.\n",
    "- `train()`, `eval()`: Sets the module in training/evaluation mode.\n",
    "- Many other utility methods for registering hooks, buffers, and parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../asssets/multilayer-perceptron-with-2-hidden-layers.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (layers): Sequential(\n",
      "    (0): Linear(in_features=50, out_features=30, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=30, out_features=20, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=20, out_features=3, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "class NeuralNetwork(torch.nn.Module):\n",
    "    def __init__(self, number_of_inputs: int, number_of_outputs: int):\n",
    "        super().__init__()\n",
    "        self.layers = torch.nn.Sequential(\n",
    "            # 1st Hidden Layer\n",
    "            torch.nn.Linear(in_features=number_of_inputs, out_features=30),\n",
    "            torch.nn.ReLU(),\n",
    "            # 2nd Hidden Layer\n",
    "            torch.nn.Linear(in_features=30, out_features=20),\n",
    "            torch.nn.ReLU(),\n",
    "            # Output Layer\n",
    "            torch.nn.Linear(in_features=20, out_features=number_of_outputs),\n",
    "        )\n",
    "\n",
    "    # Shape of Input Tensor: [Batch Size, Number of Inputs]\n",
    "    def forward(self, input_tensor: torch.Tensor) -> torch.Tensor:\n",
    "        logits = self.layers(input_tensor)\n",
    "        return logits\n",
    "\n",
    "\n",
    "model = NeuralNetwork(number_of_inputs=50, number_of_outputs=3)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note:** that we used the *`Sequential Class`* when defining our NeuralNetwork Class because it makes our life easier if we have a series of Layers we want to execute in a specific order, as we are doing here. This way, after instantiating *`self.layers = torch.nn.Sequential()`*, in the *`__init__`* constructor, we just have to now call the *`self.layers`* attribute instead of calling each layer individually in the *`NeuralNetwork's forward method`*.\n",
    "\n",
    "To check the *`Total Number of Trainable Parameters`* in our Model, we can use the `parameters()` Method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2213\n"
     ]
    }
   ],
   "source": [
    "total_number_of_parameters = sum(\n",
    "    p.numel() for p in model.parameters() if p.requires_grad\n",
    ")\n",
    "print(total_number_of_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note:** Each parameter for which `requires_grad=True` counts as one `trainable parameter`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-from-scratch-GfHW_nTs-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
